{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from joblib import dump, load\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec \n",
    "model = Word2Vec.load(\"data/wemb_models/word2vec/word2vec.model\")\n",
    "wordvectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = pd.read_csv(r'data/queries/qrels.txt', sep='\\t', header=None)\n",
    "query_ids = qrels[0].to_numpy()\n",
    "table_ids = qrels[2].to_numpy()\n",
    "queries = pd.read_csv(r'data/queries/queries.txt', header=None)\n",
    "queries = pd.DataFrame([row[0][row[0].find(' ') + 1:] for index, row in queries.iterrows()]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw word extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nlp = en_core_web_sm.load()\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def processString(string):\n",
    "    sentence = nlp(string)\n",
    "    wordlist = [token.text.lower() for token in sentence if not token.is_stop and not token.is_punct]\n",
    "    wordlist = [word for word in wordlist if not word in sw]\n",
    "    return wordlist\n",
    "\n",
    "def processHeaders(headers):\n",
    "    header_tokens = []\n",
    "    for header in headers:\n",
    "        match_obj = re.search(r'\\[[^\\|]*\\|(.*)\\]', header)\n",
    "        if match_obj is not None:\n",
    "            string = match_obj.group(1)\n",
    "        else:\n",
    "            string = header\n",
    "        header_tokens = header_tokens + processString(string)\n",
    "        \n",
    "    return header_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process each needed table such that we have the needed tokens per table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table_tokens_noheaders.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feat = []\n",
    "\n",
    "for table_id in table_ids:\n",
    "    table_file_id = table_id.split('-')[1]\n",
    "    if int(table_id.split('-')[2]) >= 1000:\n",
    "        table_file_id = str(int(table_file_id) - 1)\n",
    "    tables = pd.read_json(r'data/tables/re_tables-' + str(table_file_id) + '.json')\n",
    "    table = tables[table_id]\n",
    "    stringslist = processString(table.pgTitle) + processString(table.secondTitle) + processString(table.caption)\n",
    "    word_feat.append(stringslist)\n",
    "    \n",
    "\n",
    "dump(word_feat, 'table_tokens_noheaders.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process each needed query such that we have the needed tokens per table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['query_tokens.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feat = []\n",
    "\n",
    "for query in queries:\n",
    "    tokens = processString(query[0])\n",
    "    word_feat.append(list(set(tokens)))\n",
    "    \n",
    "dump(word_feat, 'query_tokens.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file = get_tmpfile(\"test.txt\")\n",
    "_ = glove2word2vec(\"data/wemb_models/glove.6B/glove.6B.50d.txt\",tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('office', 0.7581614851951599), ('senate', 0.7204986214637756), ('room', 0.7149738669395447), ('houses', 0.6888046264648438), ('capitol', 0.6851759552955627), ('building', 0.684728741645813), ('home', 0.6720309853553772), ('clinton', 0.6707026958465576), ('congressional', 0.669257640838623), ('mansion', 0.665092408657074)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['house'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert every term for both queries and tables to semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lending',\n",
       " 'refinancing',\n",
       " 'rates',\n",
       " 'main',\n",
       " 'date',\n",
       " 'eurozone',\n",
       " 'operations',\n",
       " 'marginal',\n",
       " 'facility',\n",
       " 'deposit',\n",
       " 'interest']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def projectTerms(gensim_model, terms):\n",
    "    return [projectSubTerms(gensim_model,term_list) for term_list in terms]\n",
    "\n",
    "def projectSubTerms(gensim_model, terms):\n",
    "    return [gensim_model[term] for term in terms if term in gensim_model]          \n",
    "\n",
    "query_terms = load('query_tokens.joblib')\n",
    "semantic_query_terms = projectTerms(model,query_terms)\n",
    "\n",
    "table_terms = load('table_tokens.joblib')\n",
    "semantic_table_terms = projectTerms(model,table_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_early_sim(table_terms, query_terms):\n",
    "    return cosine_similarity(np.mean(table_terms, axis = 0), np.mean(query_terms, axis = 0))\n",
    "                             \n",
    "def cos_late_max_sim(table_terms, query_terms):\n",
    "    combs = []\n",
    "    for table_term in table_terms:\n",
    "        for query_term in query_terms:\n",
    "            combs.append(cosine_similarity(table_term, query_term))\n",
    "    return np.max(combs)\n",
    "                             \n",
    "def cos_late_min_sim(table_terms, query_terms):\n",
    "    combs = []\n",
    "    for table_term in table_terms:\n",
    "        for query_term in query_terms:\n",
    "            combs.append(cosine_similarity(table_term, query_term))\n",
    "    return np.min(combs)\n",
    "                             \n",
    "def cos_late_avg_sim(table_terms, query_terms):\n",
    "    combs = []\n",
    "    for table_term in table_terms:\n",
    "        for query_term in query_terms:\n",
    "            combs.append(cosine_similarity(table_term, query_term))\n",
    "    return np.avg(combs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
