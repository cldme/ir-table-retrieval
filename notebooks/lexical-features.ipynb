{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Features (table-retrieval LTR baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_json(r'data/tables/re_tables-0875.json')\n",
    "features = pd.read_csv(r'data/features/features.txt')\n",
    "qrels = pd.read_csv(r'data/queries/qrels.txt', sep='\\t', header=None)\n",
    "queries = pd.read_csv(r'data/queries/queries.txt', header=None)\n",
    "queries = pd.DataFrame([row[0][row[0].find(' ') + 1:] for index, row in queries.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stop words from nltk english corpus\n",
    "sw = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(features.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table ids for later use\n",
    "table_ids = features['table_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the data using pandas get_dummies\n",
    "features = pd.get_dummies(features, columns = ['table_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(['query', 'max', 'sum', 'avg', 'sim', 'emax', 'esum', 'eavg', 'esim', 'cmax', 'csum', 'cavg', 'csim', 'remax', 'resum', 'reavg', 'resim'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels from features (labels are the values we want to predict)\n",
    "labels = np.array(features['rel'])\n",
    "\n",
    "# remove labels from features\n",
    "features.drop(['rel'], axis = 1, inplace = True)\n",
    "\n",
    "# save feature names for later use\n",
    "feature_columns = list(features.columns)\n",
    "\n",
    "# convert features to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Features Shape: {train_features.shape}')\n",
    "print(f'Training Labels Shape: {train_labels.shape}')\n",
    "print(f'Testing Features Shape: {test_features.shape}')\n",
    "print(f'Testing Labels Shape: {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (random forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, max_depth=3, n_jobs = 10, random_state = 42)\n",
    "\n",
    "# train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "# random_forest_1000_none.joblib: n_estimators = 1000, max_depth = None\n",
    "# random_forest_1000_3.joblib: n_estimators = 1000, max_depth = 3\n",
    "dump(rf, 'random_forest_1000_3.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "rf = load('random_forest.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create the parameter grid\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'n_estimators': [100, 500, 1000, 1500, 2000]\n",
    "}\n",
    "\n",
    "# create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the grid search to the data\n",
    "grid_search.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predictions = rf.predict(test_features)\n",
    "print(f'mean square error  : {metrics.mean_squared_error(test_labels, predictions)}')\n",
    "print(f'mean absolute error: {metrics.mean_absolute_error(test_labels, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(features)\n",
    "print(f'mean square error  : {metrics.mean_squared_error(labels, predictions)}')\n",
    "print(f'mean absolute error: {metrics.mean_absolute_error(labels, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results in trec_eval format\n",
    "raw = pd.read_csv(r'data/features/features.txt')\n",
    "res = {\n",
    "    'query-id': list(),\n",
    "    'q0': list(),\n",
    "    'document-id': list(),\n",
    "    'rank': list(),\n",
    "    'score': list(),\n",
    "    'name': list()\n",
    "}\n",
    "\n",
    "for index, row in raw.iterrows():\n",
    "    res['query-id'].append(row['query_id'])\n",
    "    res['q0'].append('Q0')\n",
    "    res['document-id'].append(row['table_id'])\n",
    "    res['rank'].append(0)\n",
    "    res['score'].append(predictions[index])\n",
    "    res['name'].append('STANDARD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame.from_dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to .txt file (for running trec_eval comparison)\n",
    "df_res.to_csv('results_1000_3.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# QLEN\n",
    "def get_qlen(query):\n",
    "    return len(query.split(' '))\n",
    "\n",
    "# IDF\n",
    "def get_idf(query, field):\n",
    "    # instantiate count vectorizer\n",
    "    cv=CountVectorizer(field, stop_words=sw)\n",
    "    # this steps generates word counts for the words in your docs\n",
    "    word_count_vector=cv.fit_transform(field)\n",
    "    # instantiate tfidf transformer (with use_idf true in order to compute idf scores)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    # compute the idf scores\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    # compute the sum of idf scores for all query terms\n",
    "    score = sum([tfidf_transformer.idf_[cv.get_feature_names().index(term)] for term in query.split(' ')])\n",
    "    # return idf score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import pageviewapi.period\n",
    "from wikitables import import_tables\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "for i in range(875, 876):\n",
    "    prefix = '0' * (4 - len(str(i)))\n",
    "    file = prefix + str(i)\n",
    "    table = 'data/tables/re_tables-' + file +'.json'\n",
    "    tmp = pd.read_json(table)\n",
    "    tmpT = tmp.T\n",
    "    \n",
    "    page_titles = set()\n",
    "    section_titles = set()\n",
    "    table_captions = set()\n",
    "    table_bodies = set()\n",
    "    \n",
    "    features = {\n",
    "        'table_id': list(),\n",
    "        'rows': list(),\n",
    "        'cols': list(),\n",
    "        'nulls': list(),\n",
    "        'inlinks': list(),\n",
    "        'outlinks': list(),\n",
    "        'views': list(),\n",
    "        'table_imp': list(),\n",
    "        'table_fraction': list()\n",
    "    }\n",
    "    \n",
    "    i = 0\n",
    "    for index, row in tmpT.iterrows():\n",
    "        table_id = _tmp.iloc[0].index[i]\n",
    "        rows = row['numDataRows']\n",
    "        cols = row['numCols']\n",
    "        title = row['pgTitle']\n",
    "        caption = row['caption']\n",
    "        data = row['data']\n",
    "        section_title = [item.lower() for item in row['title']]\n",
    "        section_titles.update(section_title)\n",
    "        \n",
    "        inlinks = 0; outlinks = 0; views = 0; table_imp = 0; text_len = 0; chars = 0; nulls = 0\n",
    "        \n",
    "        for entry in data:\n",
    "            for item in entry:\n",
    "                table_bodies.update({item.lower()})\n",
    "                if len(item) == 0:\n",
    "                    nulls += 1\n",
    "                chars += len(item)\n",
    "                \n",
    "        page = wiki.page(title)\n",
    "        if page.exists():\n",
    "            inlinks = len(page.backlinks)\n",
    "            outlinks = len(page.links)\n",
    "            views = pageviewapi.period.sum_last('en.wikipedia', title, last=365, access='all-access', agent='all-agents')\n",
    "            table_imp = 1 / (len(import_tables(title)) + 1)\n",
    "            text_len = len(page.text)\n",
    "        \n",
    "        table_fraction = chars / (text_len + 1)\n",
    "        \n",
    "        page_titles.update({title.lower()})\n",
    "        table_captions.update({caption.lower()})\n",
    "        \n",
    "        features['table_id'].append(table_id)\n",
    "        features['rows'].append(rows)\n",
    "        features['cols'].append(cols)\n",
    "        features['nulls'].append(nulls)\n",
    "        features['inlinks'].append(inlinks)\n",
    "        features['outlinks'].append(outlinks)\n",
    "        features['views'].append(views)\n",
    "        features['table_imp'].append(table_imp)\n",
    "        features['table_fraction'].append(table_fraction)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)\n",
    "print(page_titles)\n",
    "print(section_titles)\n",
    "print(table_captions)\n",
    "print(table_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson.bigjson as bj\n",
    "\n",
    "with open('tables.json', 'rb') as f:\n",
    "    reader = bj.FileReader(f, 'utf-8')\n",
    "    i = reader.read(True, False)\n",
    "    \n",
    "print(i.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tableMentions.json', 'rb') as f:\n",
    "    reader = bj.FileReader(f, 'utf-8')\n",
    "    j = reader.read(True, True)\n",
    "    \n",
    "print(j.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
